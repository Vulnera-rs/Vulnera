//! Use case for explaining vulnerabilities in natural language

use crate::domain::{CompletionRequest, LlmError, LlmProvider, StreamChunk};
use crate::infrastructure::prompts::PromptBuilder;
use futures::stream::BoxStream;
use std::sync::Arc;
use vulnera_core::config::LlmConfig;

pub struct ExplainVulnerabilityUseCase {
    provider: Arc<dyn LlmProvider>,
    config: LlmConfig,
}

impl ExplainVulnerabilityUseCase {
    pub fn new(provider: Arc<dyn LlmProvider>, config: LlmConfig) -> Self {
        Self { provider, config }
    }

    /// Execute with streaming response
    pub async fn execute_stream(
        &self,
        finding_id: &str,
        severity: &str,
        description: &str,
    ) -> Result<BoxStream<'static, Result<StreamChunk, LlmError>>, LlmError> {
        let model = self
            .config
            .explanation_model
            .as_deref()
            .unwrap_or(&self.config.default_model);

        let user_prompt =
            PromptBuilder::build_explanation_prompt(finding_id, severity, description);

        let request = CompletionRequest::new()
            .with_model(model)
            .with_user(user_prompt)
            .with_max_tokens(self.config.max_tokens)
            .with_temperature(self.config.temperature)
            .with_stream(true);

        self.provider.complete_stream(request).await
    }

    /// Execute without streaming (returns full response)
    pub async fn execute(
        &self,
        finding_id: &str,
        severity: &str,
        description: &str,
    ) -> Result<String, LlmError> {
        let model = self
            .config
            .explanation_model
            .as_deref()
            .unwrap_or(&self.config.default_model);

        let user_prompt =
            PromptBuilder::build_explanation_prompt(finding_id, severity, description);

        let request = CompletionRequest::new()
            .with_model(model)
            .with_user(user_prompt)
            .with_max_tokens(self.config.max_tokens)
            .with_temperature(self.config.temperature);

        let response = self.provider.complete(request).await?;
        Ok(response.text())
    }
}
