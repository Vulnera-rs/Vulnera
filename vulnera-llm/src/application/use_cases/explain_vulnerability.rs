use crate::domain::{LlmRequest, LlmResponse, Message};
use crate::infrastructure::prompts::PromptBuilder;
use crate::infrastructure::providers::LlmProvider;
use std::sync::Arc;
use tokio::sync::mpsc;
use vulnera_core::config::LlmConfig;

pub struct ExplainVulnerabilityUseCase {
    provider: Arc<dyn LlmProvider>,
    config: LlmConfig,
}

impl ExplainVulnerabilityUseCase {
    pub fn new(provider: Arc<dyn LlmProvider>, config: LlmConfig) -> Self {
        Self { provider, config }
    }

    pub async fn execute_stream(
        &self,
        finding_id: &str,
        severity: &str,
        description: &str,
    ) -> Result<mpsc::Receiver<Result<LlmResponse, anyhow::Error>>, anyhow::Error> {
        let model = self
            .config
            .explanation_model
            .as_deref()
            .unwrap_or(&self.config.default_model);
        let user_prompt =
            PromptBuilder::build_explanation_prompt(finding_id, severity, description);

        let request = LlmRequest {
            model: model.to_string(),
            messages: vec![Message::new("user", user_prompt)],
            max_tokens: Some(self.config.max_tokens),
            temperature: Some(self.config.temperature),
            top_p: Some(0.95),
            top_k: None,
            frequency_penalty: None,
            presence_penalty: None,
            stream: Some(true),
        };

        self.provider.generate_stream(request).await
    }
}
